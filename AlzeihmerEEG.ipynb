{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_eeg_data(data_dir):\n",
    "    rows = []\n",
    "\n",
    "    for condition in ['Healthy', 'AD']:\n",
    "        for state in ['Eyes_open', 'Eyes_closed']:\n",
    "            folder_path = os.path.join(data_dir, condition, state)\n",
    "            if not os.path.exists(folder_path):\n",
    "                print(f\"Directory {folder_path} not found.\")\n",
    "                continue\n",
    "\n",
    "            for paciente in os.listdir(folder_path):\n",
    "                paciente_folder = os.path.join(folder_path, paciente)\n",
    "                if os.path.isdir(paciente_folder):\n",
    "                    for file in os.listdir(paciente_folder):\n",
    "                        if file.endswith('.txt'):\n",
    "                            file_path = os.path.join(paciente_folder, file)\n",
    "                            try:\n",
    "                                data = np.loadtxt(file_path)\n",
    "                                rows.append({\n",
    "                                    'Condition': condition,\n",
    "                                    'State': state,\n",
    "                                    'Patient': paciente,\n",
    "                                    'File': file,\n",
    "                                    'Signal': data\n",
    "                                })\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    eeg_df = pd.DataFrame(rows)\n",
    "    print(\"EEG data loaded successfully. Number of records:\", len(eeg_df))\n",
    "    return eeg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Étape 1 : Charger les données EEG\n",
    "data_dir = \"./EEG_data\"\n",
    "eeg_data_df = load_eeg_data(data_dir)\n",
    "\n",
    "# Étape 2 : Nettoyage des données\n",
    "def clean_data(df):\n",
    "    # Supprimer les lignes avec des valeurs manquantes dans le signal\n",
    "    df = df.dropna(subset=['Signal'])\n",
    "    print(\"Valeurs manquantes supprimées. Nombre de lignes restantes :\", len(df))\n",
    "\n",
    "    # Filtrer les valeurs aberrantes : par exemple, en supprimant les signaux avec des amplitudes extrêmes\n",
    "    df = df[df['Signal'].apply(lambda x: np.all(np.abs(x) < 1000))]  # Exemple de seuil\n",
    "    print(\"Valeurs aberrantes filtrées. Nombre de lignes restantes :\", len(df))\n",
    "\n",
    "    # Normaliser les données si nécessaire\n",
    "    df['Signal'] = df['Signal'].apply(lambda x: (x - np.mean(x)) / np.std(x))\n",
    "    print(\"Les signaux ont été normalisés.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Appliquer le nettoyage des données\n",
    "eeg_data_cleaned = clean_data(eeg_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Afficher les données pour les premiers patients\n",
    "print(eeg_data_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Charger les données brutes EEG (supposons que eeg_data_cleaned soit votre DataFrame contenant les signaux bruts)\n",
    "# Nous allons utiliser la colonne 'Signal' comme données d'entrée\n",
    "\n",
    "# Préparation des données\n",
    "def prepare_raw_data(df):\n",
    "    # Les signaux EEG bruts sont des séries de valeurs. Pour simplifier, nous allons les aplatir.\n",
    "    X = np.vstack(df['Signal'].values)  # Transformer la colonne 'Signal' en matrice d'entrée\n",
    "    y = df['Condition'].apply(lambda x: 1 if x == 'AD' else 0)  # 1 pour AD, 0 pour Healthy\n",
    "    return X, y\n",
    "\n",
    "# Appliquer la préparation\n",
    "X, y = prepare_raw_data(eeg_data_cleaned)\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Créer et entraîner le modèle d'arbre de décision\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Modèle d'arbre de décision entraîné avec succès.\")\n",
    "\n",
    "# Prédire les résultats sur l'ensemble de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Afficher le rapport de classification et la précision\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Affichage de l'arbre de décision\n",
    "from sklearn.tree import export_text\n",
    "tree_text = export_text(model)\n",
    "print(\"Arbre de décision :\\n\", tree_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Afficher l'arbre de décision\n",
    "plt.figure(figsize=(20, 10))  # Ajuster la taille de l'image\n",
    "plot_tree(\n",
    "    model,\n",
    "    feature_names=[f'Feature {i}' for i in range(X.shape[1])],  # Utilisation de noms génériques pour les caractéristiques\n",
    "    class_names=['Healthy', 'AD'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(\"Arbre de décision - EEG Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Afficher le rapport de classification\n",
    "print(\"Rapport de classification - Arbre de Décision :\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculer l'accuracy sur les données de test\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Afficher l'accuracy\n",
    "print(f\"Accuracy du modèle : {accuracy:.2f}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculer la matrice de confusion\n",
    "cm_decision_tree = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Afficher la matrice de confusion\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_decision_tree, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=['Healthy', 'AD'], yticklabels=['Healthy', 'AD'])\n",
    "plt.xlabel(\"Prédiction\")\n",
    "plt.ylabel(\"Vraie Classe\")\n",
    "plt.title(\"Matrice de Confusion - Arbre de Décision\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Créer et entraîner le modèle de forêt aléatoire\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "print(\"Modèle de forêt aléatoire entraîné avec succès.\")\n",
    "\n",
    "# Prédire les résultats sur l'ensemble de test\n",
    "y_pred_rf = random_forest_model.predict(X_test)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Sélectionner un arbre de la forêt aléatoire (par exemple, le premier arbre)\n",
    "plt.figure(figsize=(20, 10))  # Ajuster la taille de l'image\n",
    "plot_tree(\n",
    "    random_forest_model.estimators_[0],  # Premier arbre de la forêt\n",
    "    feature_names=[f'Feature {i}' for i in range(X.shape[1])],  # Utiliser les noms de caractéristiques génériques\n",
    "    class_names=['Healthy', 'AD'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(\"Affichage de l'arbre - Forêt Aléatoire\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Afficher le rapport de classification\n",
    "print(\"Rapport de classification - Forêt Aléatoire :\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"Accuracy (précision) de la Forêt Aléatoire :\", accuracy_score(y_test, y_pred_rf))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculer la matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# Afficher la matrice de confusion sous forme graphique\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Healthy', 'AD'], yticklabels=['Healthy', 'AD'])\n",
    "plt.xlabel(\"Prédiction\")\n",
    "plt.ylabel(\"Vraie Classe\")\n",
    "plt.title(\"Matrice de Confusion - Forêt Aléatoire\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Étape 1 : Créer et entraîner le modèle KNN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)  # Nombre de voisins ajustable\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(\"Modèle KNN entraîné avec succès.\")\n",
    "\n",
    "# Étape 2 : Prédire les résultats sur l'ensemble de test\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Étape 3 : Afficher les premières prédictions et les étiquettes réelles correspondantes\n",
    "print(\"Prédictions KNN sur les premières données de test :\")\n",
    "for i in range(5):  # Afficher les 5 premières prédictions\n",
    "    print(f\"Prédiction : {y_pred_knn[i]}, Véritable étiquette : {y_test.iloc[i]}\")\n",
    "\n",
    "# Étape 4 : Calculer et afficher l'accuracy\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"Accuracy du modèle KNN : {accuracy_knn:.2f}\")\n",
    "\n",
    "# Étape 5 : Afficher le rapport de classification\n",
    "print(\"Rapport de classification - KNN :\\n\", classification_report(y_test, y_pred_knn))\n",
    "\n",
    "# Étape 6 : Matrice de Confusion\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "\n",
    "# Afficher la matrice de confusion pour KNN\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_knn, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Healthy', 'AD'], yticklabels=['Healthy', 'AD'])\n",
    "plt.xlabel(\"Prédiction\")\n",
    "plt.ylabel(\"Vraie Classe\")\n",
    "plt.title(\"Matrice de Confusion - KNN\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Étape 1 : Créer et entraîner le modèle SVM\n",
    "svm_model = SVC(kernel='linear', random_state=42)  # Utilisation d'un noyau linéaire\n",
    "svm_model.fit(X_train, y_train)\n",
    "print(\"Modèle SVM entraîné avec succès.\")\n",
    "\n",
    "# Étape 2 : Prédire les résultats sur l'ensemble de test\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "# Étape 3 : Afficher les premières prédictions et les étiquettes réelles correspondantes\n",
    "print(\"Prédictions SVM sur les premières données de test :\")\n",
    "for i in range(5):  # Afficher les 5 premières prédictions\n",
    "    print(f\"Prédiction : {y_pred_svm[i]}, Véritable étiquette : {y_test.iloc[i]}\")\n",
    "\n",
    "# Étape 4 : Calculer et afficher l'accuracy\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"Accuracy du modèle SVM : {accuracy_svm:.2f}\")\n",
    "\n",
    "# Étape 5 : Afficher le rapport de classification\n",
    "print(\"Rapport de classification - SVM :\\n\", classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Étape 6 : Matrice de Confusion\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "# Afficher la matrice de confusion pour SVM\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_svm, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=['Healthy', 'AD'], yticklabels=['Healthy', 'AD'])\n",
    "plt.xlabel(\"Prédiction\")\n",
    "plt.ylabel(\"Vraie Classe\")\n",
    "plt.title(\"Matrice de Confusion - SVM\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Fonction pour extraire des caractéristiques statistiques\n",
    "def extract_statistical_features(df):\n",
    "    features = df['Signal'].apply(lambda x: pd.Series({\n",
    "        'mean': np.mean(x),\n",
    "        'std_dev': np.std(x),\n",
    "        'min': np.min(x),\n",
    "        'max': np.max(x),\n",
    "        'median': np.median(x),\n",
    "        'range': np.max(x) - np.min(x),       # Amplitude\n",
    "        'variance': np.var(x),                # Variance\n",
    "        'q1': np.percentile(x, 25),           # Premier quartile\n",
    "        'q3': np.percentile(x, 75),           # Troisième quartile\n",
    "        'entropy': entropy(np.histogram(x, bins=10)[0])  # Entropie\n",
    "    }))\n",
    "    return features\n",
    "\n",
    "# Extraire les caractéristiques et les ajouter au DataFrame\n",
    "statistical_features = extract_statistical_features(eeg_data_cleaned)\n",
    "eeg_data_with_stats = pd.concat([eeg_data_cleaned, statistical_features], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Afficher l'arbre de décision\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    decision_tree_model,\n",
    "    feature_names=X_stats.columns,\n",
    "    class_names=['Healthy', 'AD'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(\"Arbre de décision - Caractéristiques statistiques\")\n",
    "plt.show()\n",
    "\n",
    "# Prédire les résultats sur l'ensemble de test\n",
    "y_pred_dt_stats = decision_tree_model.predict(X_test_stats)\n",
    "\n",
    "# Calculer l'accuracy\n",
    "accuracy_dt_stats = accuracy_score(y_test_stats, y_pred_dt_stats)\n",
    "print(f\"Accuracy de l'arbre de décision (caractéristiques statistiques) : {accuracy_dt_stats:.2f}\")\n",
    "\n",
    "# Afficher le rapport de classification\n",
    "report = classification_report(y_test_stats, y_pred_dt_stats, target_names=['Healthy', 'AD'])\n",
    "print(\"\\nRapport de classification :\\n\", report)\n",
    "\n",
    "# Afficher la matrice de confusion\n",
    "conf_matrix = confusion_matrix(y_test_stats, y_pred_dt_stats)\n",
    "print(\"\\nMatrice de confusion :\\n\", conf_matrix)\n",
    "\n",
    "# Visualiser la matrice de confusion\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(\"Matrice de confusion - Arbre de décision\")\n",
    "plt.colorbar()\n",
    "tick_marks = [0, 1]\n",
    "plt.xticks(tick_marks, ['Healthy', 'AD'])\n",
    "plt.yticks(tick_marks, ['Healthy', 'AD'])\n",
    "plt.xlabel('Prédiction')\n",
    "plt.ylabel('Vérité terrain')\n",
    "\n",
    "# Ajouter les valeurs de la matrice de confusion\n",
    "for i in range(len(conf_matrix)):\n",
    "    for j in range(len(conf_matrix[i])):\n",
    "        plt.text(j, i, str(conf_matrix[i][j]), horizontalalignment='center', color='white' if conf_matrix[i, j] > conf_matrix.max() / 2 else 'black')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Créer et entraîner le modèle de forêt aléatoire\n",
    "random_forest_model_stats = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest_model_stats.fit(X_train_stats, y_train_stats)\n",
    "print(\"Modèle de forêt aléatoire (caractéristiques statistiques) entraîné avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Afficher un arbre de la forêt aléatoire\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    random_forest_model_stats.estimators_[0],\n",
    "    feature_names=X_stats.columns,\n",
    "    class_names=['Healthy', 'AD'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(\"Arbre de la forêt aléatoire - Caractéristiques statistiques\")\n",
    "plt.show()\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred_stats = random_forest_model_stats.predict(X_test_stats)\n",
    "\n",
    "# Générer le rapport de classification\n",
    "report = classification_report(y_test_stats, y_pred_stats, target_names=['Healthy', 'AD'])\n",
    "print(\"Rapport de classification :\\n\", report)\n",
    "\n",
    "# Matrice de confusion\n",
    "conf_matrix = confusion_matrix(y_test_stats, y_pred_stats)\n",
    "print(\"Matrice de confusion :\\n\", conf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Créer et entraîner le modèle KNN\n",
    "knn_model_stats = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model_stats.fit(X_train_stats, y_train_stats)\n",
    "print(\"Modèle KNN (caractéristiques statistiques) entraîné avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred_knn_stats = knn_model_stats.predict(X_test_stats)\n",
    "\n",
    "# Calculer l'accuracy\n",
    "accuracy_knn_stats = accuracy_score(y_test_stats, y_pred_knn_stats)\n",
    "print(f\"Accuracy du modèle KNN (caractéristiques statistiques) : {accuracy_knn_stats:.2f}\")\n",
    "\n",
    "# Générer le rapport de classification\n",
    "report_knn = classification_report(y_test_stats, y_pred_knn_stats, target_names=['Healthy', 'AD'])\n",
    "print(\"Rapport de classification KNN :\\n\", report_knn)\n",
    "\n",
    "# Matrice de confusion\n",
    "conf_matrix_knn = confusion_matrix(y_test_stats, y_pred_knn_stats)\n",
    "print(\"Matrice de confusion KNN :\\n\", conf_matrix_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Créer et entraîner le modèle SVM\n",
    "svm_model_stats = SVC(kernel='linear', random_state=42)\n",
    "svm_model_stats.fit(X_train_stats, y_train_stats)\n",
    "print(\"Modèle SVM (caractéristiques statistiques) entraîné avec succès.\")\n",
    "\n",
    "# Afficher quelques prédictions SVM\n",
    "print(\"Prédictions SVM (caractéristiques statistiques) sur les premières données de test :\")\n",
    "for i in range(5):\n",
    "    print(f\"Prédiction : {svm_model_stats.predict([X_test_stats.iloc[i]])[0]}, Véritable étiquette : {y_test_stats.iloc[i]}\")\n",
    "\n",
    "# Prédire les résultats\n",
    "y_pred_svm_stats = svm_model_stats.predict(X_test_stats)\n",
    "\n",
    "# Calculer l'accuracy\n",
    "accuracy_svm_stats = accuracy_score(y_test_stats, y_pred_svm_stats)\n",
    "print(f\"Accuracy du modèle SVM (caractéristiques statistiques) : {accuracy_svm_stats:.2f}\")\n",
    "\n",
    "# Générer le rapport de classification\n",
    "report_svm = classification_report(y_test_stats, y_pred_svm_stats, target_names=['Healthy', 'AD'])\n",
    "print(\"Rapport de classification SVM :\\n\", report_svm)\n",
    "\n",
    "# Matrice de confusion\n",
    "conf_matrix_svm = confusion_matrix(y_test_stats, y_pred_svm_stats)\n",
    "print(\"Matrice de confusion SVM :\\n\", conf_matrix_svm)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Préparer les données : normaliser les caractéristiques\n",
    "X_data = np.vstack(eeg_data_cleaned['Signal'].values)  # Utiliser les données brutes\n",
    "X_data = (X_data - np.mean(X_data)) / np.std(X_data)   # Normalisation\n",
    "\n",
    "# Ajouter du bruit aux données pour le Denoising Autoencoder\n",
    "noise_factor = 0.5\n",
    "X_noisy = X_data + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_data.shape)\n",
    "\n",
    "# Définir les dimensions\n",
    "input_dim = X_data.shape[1]\n",
    "encoding_dim = 64  # Taille de la couche latente\n",
    "\n",
    "# Définir le modèle Denoising Autoencoder\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "# Créer le modèle autoencoder\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Entraîner le modèle\n",
    "autoencoder.fit(X_noisy, X_data, epochs=50, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "print(\"Denoising Autoencoder entraîné avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Définir le modèle encodeur pour extraire les caractéristiques\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "# Extraire les caractéristiques\n",
    "X_encoded = encoder.predict(X_data)\n",
    "print(\"Caractéristiques extraites avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Diviser les données\n",
    "X_train_enc, X_test_enc, y_train_enc, y_test_enc = train_test_split(X_encoded, y_stats, test_size=0.3, random_state=42)\n",
    "\n",
    "# Créer et entraîner le modèle d'arbre de décision\n",
    "decision_tree_model_enc = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree_model_enc.fit(X_train_enc, y_train_enc)\n",
    "print(\"Modèle d'arbre de décision (autoencoder) entraîné avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Afficher l'arbre de décision\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    decision_tree_model_enc,\n",
    "    feature_names=[f'Feature {i}' for i in range(X_encoded.shape[1])],\n",
    "    class_names=['Healthy', 'AD'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(\"Arbre de décision - Autoencoder\")\n",
    "plt.show()\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prédire les résultats\n",
    "y_pred_dt_enc = decision_tree_model_enc.predict(X_test_enc)\n",
    "\n",
    "# Calculer l'accuracy\n",
    "accuracy_dt_enc = accuracy_score(y_test_enc, y_pred_dt_enc)\n",
    "print(f\"Accuracy de l'arbre de décision (autoencoder) : {accuracy_dt_enc:.2f}\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Rapport de classification\n",
    "report = classification_report(y_test_enc, y_pred_dt_enc, target_names=['Healthy', 'AD'])\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Créer et entraîner le modèle de forêt aléatoire\n",
    "random_forest_model_enc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest_model_enc.fit(X_train_enc, y_train_enc)\n",
    "print(\"Modèle de forêt aléatoire (autoencoder) entraîné avec succès.\")\n",
    "# Afficher un arbre de la forêt aléatoire\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    random_forest_model_enc.estimators_[0],\n",
    "    feature_names=[f'Feature {i}' for i in range(X_encoded.shape[1])],\n",
    "    class_names=['Healthy', 'AD'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(\"Arbre de la forêt aléatoire - Autoencoder\")\n",
    "plt.show()\n",
    "\n",
    "# Prédire les résultats\n",
    "y_pred_rf_enc = random_forest_model_enc.predict(X_test_enc)\n",
    "\n",
    "# Calculer l'accuracy\n",
    "accuracy_rf_enc = accuracy_score(y_test_enc, y_pred_rf_enc)\n",
    "print(f\"Accuracy de la forêt aléatoire (autoencoder) : {accuracy_rf_enc:.2f}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Rapport de classification\n",
    "report = classification_report(y_test_enc, y_pred_rf_enc, target_names=['Healthy', 'AD'])\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Créer et entraîner le modèle SVM\n",
    "svm_model_enc = SVC(kernel='linear', random_state=42)\n",
    "svm_model_enc.fit(X_train_enc, y_train_enc)\n",
    "print(\"Modèle SVM (autoencoder) entraîné avec succès.\")\n",
    "\n",
    "# Afficher quelques prédictions SVM\n",
    "print(\"Prédictions SVM (autoencoder) sur les premières données de test :\")\n",
    "for i in range(5):\n",
    "    print(f\"Prédiction : {svm_model_enc.predict([X_test_enc[i]])[0]}, Véritable étiquette : {y_test_enc.iloc[i]}\")\n",
    "\n",
    "# Prédire les résultats\n",
    "y_pred_svm_enc = svm_model_enc.predict(X_test_enc)\n",
    "\n",
    "# Calculer l'accuracy\n",
    "accuracy_svm_enc = accuracy_score(y_test_enc, y_pred_svm_enc)\n",
    "print(f\"Accuracy du modèle SVM (autoencoder) : {accuracy_svm_enc:.2f}\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Rapport de classification\n",
    "report = classification_report(y_test_enc, y_pred_rf_enc, target_names=['Healthy', 'AD'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import mse\n",
    "\n",
    "def train_autoencoder(X_data, autoencoder_type='dae', encoding_dim=64, noise_factor=0.5):\n",
    "    input_dim = X_data.shape[1]\n",
    "\n",
    "    if autoencoder_type == 'dae':\n",
    "        # Ajouter du bruit pour Denoising Autoencoder\n",
    "        X_noisy = X_data + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_data.shape)\n",
    "        input_layer = Input(shape=(input_dim,))\n",
    "        encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "        decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "        autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "        autoencoder.compile(optimizer='adam', loss='mse')\n",
    "        autoencoder.fit(X_noisy, X_data, epochs=50, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "    elif autoencoder_type == 'vae':\n",
    "        # Variational Autoencoder\n",
    "        latent_dim = encoding_dim\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean, z_log_var = args\n",
    "            batch = K.shape(z_mean)[0]\n",
    "            dim = K.int_shape(z_mean)[1]\n",
    "            epsilon = K.random_normal(shape=(batch, dim))\n",
    "            return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "        input_layer = Input(shape=(input_dim,))\n",
    "        z_mean = Dense(latent_dim)(input_layer)\n",
    "        z_log_var = Dense(latent_dim)(input_layer)\n",
    "        z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "        encoded = Dense(latent_dim, activation='relu')(z)\n",
    "        decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "        autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        autoencoder.add_loss(K.mean(kl_loss) / input_dim)\n",
    "        autoencoder.compile(optimizer='adam', loss='mse')\n",
    "        autoencoder.fit(X_data, X_data, epochs=50, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "    elif autoencoder_type == 'cae':\n",
    "        # Contractive Autoencoder\n",
    "        input_layer = Input(shape=(input_dim,))\n",
    "        encoded = Dense(encoding_dim, activation='relu', activity_regularizer=tf.keras.regularizers.L2(1e-5))(input_layer)\n",
    "        decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "        autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "        autoencoder.compile(optimizer='adam', loss='mse')\n",
    "        autoencoder.fit(X_data, X_data, epochs=50, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "    encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "    X_encoded = encoder.predict(X_data)\n",
    "    return autoencoder, X_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def train_and_evaluate(X_encoded, y, model_type='decision_tree'):\n",
    "    # Diviser les données en ensembles d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Sélectionner le modèle\n",
    "    if model_type == 'decision_tree':\n",
    "        model = DecisionTreeClassifier(random_state=42)\n",
    "    elif model_type == 'random_forest':\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    elif model_type == 'knn':\n",
    "        model = KNeighborsClassifier(n_neighbors=5)\n",
    "    elif model_type == 'svm':\n",
    "        model = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Prédire les résultats sur l'ensemble de test\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculer et afficher l'accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy du modèle {model_type} : {accuracy:.2f}\")\n",
    "\n",
    "    return model, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def run_all_experiments(X_data, y, autoencoder_types=['dae', 'vae', 'cae'], model_types=['decision_tree', 'random_forest', 'knn', 'svm']):\n",
    "    for autoencoder_type in autoencoder_types:\n",
    "        print(f\"\\nEntraînement avec l'autoencodeur : {autoencoder_type.upper()}\")\n",
    "        # Entraîner l'autoencodeur et extraire les caractéristiques\n",
    "        _, X_encoded = train_autoencoder(X_data, autoencoder_type=autoencoder_type)\n",
    "\n",
    "        for model_type in model_types:\n",
    "            print(f\"  -> Évaluation avec le modèle : {model_type.upper()}\")\n",
    "            # Entraîner et évaluer le modèle de classification\n",
    "            _, acc = train_and_evaluate(X_encoded, y, model_type=model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "import numpy as np\n",
    "\n",
    "# Redéfinir la fonction train_autoencoder pour corriger l'entraînement du VAE\n",
    "def train_autoencoder(X_data, autoencoder_type='dae', encoding_dim=64, noise_factor=0.5):\n",
    "    input_dim = X_data.shape[1]\n",
    "\n",
    "    if autoencoder_type == 'vae':\n",
    "        # Variational Autoencoder avec une couche personnalisée pour la perte KL\n",
    "        latent_dim = encoding_dim\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean, z_log_var = args\n",
    "            batch = K.shape(z_mean)[0]\n",
    "            dim = K.int_shape(z_mean)[1]\n",
    "            epsilon = K.random_normal(shape=(batch, dim))\n",
    "            return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "        # Définir les couches du VAE\n",
    "        input_layer = Input(shape=(input_dim,))\n",
    "        z_mean = Dense(latent_dim, name='z_mean')(input_layer)\n",
    "        z_log_var = Dense(latent_dim, name='z_log_var')(input_layer)\n",
    "        z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "        encoded = Dense(latent_dim, activation='relu')(z)\n",
    "        decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "        # Créer le modèle autoencoder\n",
    "        autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "\n",
    "        # Calcul de la perte KL\n",
    "        class KLLossLayer(Layer):\n",
    "            def call(self, inputs):\n",
    "                z_mean, z_log_var = inputs\n",
    "                kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "                self.add_loss(K.mean(kl_loss) / input_dim)\n",
    "                return inputs\n",
    "\n",
    "        # Ajouter la perte KL à l'autoencodeur\n",
    "        kl_loss_layer = KLLossLayer(name='kl_loss')([z_mean, z_log_var])\n",
    "\n",
    "        # Compiler le modèle avec la perte\n",
    "        autoencoder.compile(optimizer='adam', loss='mse')\n",
    "        autoencoder.fit(X_data, X_data, epochs=50, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "        # Créer le modèle encodeur pour extraire les caractéristiques\n",
    "        encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "        X_encoded = encoder.predict(X_data)\n",
    "        return autoencoder, X_encoded\n",
    "\n",
    "    else:\n",
    "        \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def run_all_experiments(X_data, y, autoencoder_types=['dae', 'sparse', 'deep'], model_types=['decision_tree', 'random_forest', 'knn', 'svm']):\n",
    "    for autoencoder_type in autoencoder_types:\n",
    "        print(f\"\\nEntraînement avec l'autoencodeur : {autoencoder_type.upper()}\")\n",
    "        # Entraîner l'autoencodeur et extraire les caractéristiques\n",
    "        _, X_encoded = train_autoencoder(X_data, autoencoder_type=autoencoder_type)\n",
    "\n",
    "        for model_type in model_types:\n",
    "            print(f\"  -> Évaluation avec le modèle : {model_type.upper()}\")\n",
    "            # Entraîner et évaluer le modèle de classification\n",
    "            _, acc = train_and_evaluate(X_encoded, y, model_type=model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def run_all_experiments(X_data, y, autoencoder_types=['dae', 'sparse', 'deep'], model_types=['decision_tree', 'random_forest', 'knn', 'svm']):\n",
    "    for autoencoder_type in autoencoder_types:\n",
    "        print(f\"\\nEntraînement avec l'autoencodeur : {autoencoder_type.upper()}\")\n",
    "        # Entraîner l'autoencodeur et extraire les caractéristiques\n",
    "        _, X_encoded = train_autoencoder(X_data, autoencoder_type=autoencoder_type)\n",
    "\n",
    "        for model_type in model_types:\n",
    "            print(f\"  -> Évaluation avec le modèle : {model_type.upper()}\")\n",
    "            # Entraîner et évaluer le modèle de classification\n",
    "            _, acc = train_and_evaluate(X_encoded, y, model_type=model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "def train_autoencoder(X_data, autoencoder_type='dae', encoding_dim=64, noise_factor=0.5):\n",
    "    input_dim = X_data.shape[1]\n",
    "\n",
    "    if autoencoder_type == 'dae':\n",
    "        # Denoising Autoencoder\n",
    "        X_noisy = X_data + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_data.shape)\n",
    "        input_layer = Input(shape=(input_dim,))\n",
    "        encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "        decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "        autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "        autoencoder.compile(optimizer='adam', loss='mse')\n",
    "        autoencoder.fit(X_noisy, X_data, epochs=50, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "    elif autoencoder_type == 'sparse':\n",
    "        # Sparse Autoencoder avec régularisation L1\n",
    "        input_layer = Input(shape=(input_dim,))\n",
    "        encoded = Dense(encoding_dim, activation='relu', activity_regularizer=tf.keras.regularizers.l1(1e-5))(input_layer)\n",
    "        decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "        autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "        autoencoder.compile(optimizer='adam', loss='mse')\n",
    "        autoencoder.fit(X_data, X_data, epochs=50, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "    elif autoencoder_type == 'deep':\n",
    "        # Deep Autoencoder avec plusieurs couches\n",
    "        input_layer = Input(shape=(input_dim,))\n",
    "        encoded = Dense(128, activation='relu')(input_layer)\n",
    "        encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "        decoded = Dense(128, activation='relu')(encoded)\n",
    "        decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "        autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "        autoencoder.compile(optimizer='adam', loss='mse')\n",
    "        autoencoder.fit(X_data, X_data, epochs=50, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "    # Extraire les caractéristiques encodées\n",
    "    encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "    X_encoded = encoder.predict(X_data)\n",
    "    return autoencoder, X_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Préparer les données brutes et normaliser\n",
    "X_data = np.vstack(eeg_data_cleaned['Signal'].values)\n",
    "X_data = (X_data - np.mean(X_data)) / np.std(X_data)\n",
    "y = eeg_data_cleaned['Condition'].apply(lambda x: 1 if x == 'AD' else 0).values\n",
    "\n",
    "# Lancer les expériences pour les trois autoencodeurs et les modèles de classification\n",
    "run_all_experiments(X_data, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Génération de données simulées\n",
    "def generate_data(num_samples=1000, num_features=10):\n",
    "    X = np.random.rand(num_samples, num_features)\n",
    "    noise = np.random.normal(0, 0.1, X.shape)\n",
    "    X_noisy = X + noise\n",
    "    return X_noisy, X\n",
    "\n",
    "# Charger les données\n",
    "X_noisy, X = generate_data()\n",
    "X_train, X_val = train_test_split(X_noisy, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modèle DAE\n",
    "def build_dae_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Modèle épars\n",
    "def build_sparse_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(64, activation='relu', activity_regularizer=keras.regularizers.l1(0.01)),\n",
    "        layers.Dense(32, activation='relu', activity_regularizer=keras.regularizers.l1(0.01)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Entraînement des modèles\n",
    "def train_models(X_train, X_val):\n",
    "    input_shape = (X_train.shape[1],)\n",
    "\n",
    "    # DAE\n",
    "    dae_model = build_dae_model(input_shape)\n",
    "    dae_history = dae_model.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_val, X_val), verbose=0)\n",
    "\n",
    "    # Modèle épars\n",
    "    sparse_model = build_sparse_model(input_shape)\n",
    "    sparse_history = sparse_model.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_val, X_val), verbose=0)\n",
    "\n",
    "    return dae_history, sparse_history\n",
    "\n",
    "# Visualisation des pertes\n",
    "def plot_losses(dae_history, sparse_history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # DAE Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(dae_history.history['loss'], label='DAE Loss', color='blue')\n",
    "    plt.plot(dae_history.history['val_loss'], label='DAE Val Loss', color='orange')\n",
    "    plt.title('DAE Loss and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Sparse Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(sparse_history.history['loss'], label='Sparse Loss', color='blue')\n",
    "    plt.plot(sparse_history.history['val_loss'], label='Sparse Val Loss', color='orange')\n",
    "    plt.title('Sparse Loss and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Exécution de l'entraînement et de la visualisation\n",
    "dae_history, sparse_history = train_models(X_train, X_val)\n",
    "plot_losses(dae_history, sparse_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Génération de données simulées\n",
    "def generate_data(num_samples=1000, num_features=10):\n",
    "    X = np.random.rand(num_samples, num_features)\n",
    "    noise = np.random.normal(0, 0.1, X.shape)\n",
    "    X_noisy = X + noise\n",
    "    \n",
    "    # Simulating binary labels (0 or 1) based on a threshold\n",
    "    y = (X.mean(axis=1) > 0.5).astype(int)\n",
    "    return X_noisy, X, y\n",
    "\n",
    "# Charger les données\n",
    "X_noisy, X, y = generate_data()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_noisy, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modèle DAE\n",
    "def build_dae_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Modèle épars\n",
    "def build_sparse_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(64, activation='relu', activity_regularizer=keras.regularizers.l1(0.01)),\n",
    "        layers.Dense(32, activation='relu', activity_regularizer=keras.regularizers.l1(0.01)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Entraînement des modèles\n",
    "def train_models(X_train, X_val):\n",
    "    input_shape = (X_train.shape[1],)\n",
    "\n",
    "    # DAE\n",
    "    dae_model = build_dae_model(input_shape)\n",
    "    dae_history = dae_model.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_val, X_val), verbose=0)\n",
    "\n",
    "    # Modèle épars\n",
    "    sparse_model = build_sparse_model(input_shape)\n",
    "    sparse_history = sparse_model.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_val, X_val), verbose=0)\n",
    "\n",
    "    return dae_history, sparse_history\n",
    "\n",
    "# Entraîner le modèle de régression logistique\n",
    "def train_logistic_regression(X_train, y_train):\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Visualisation des précisions\n",
    "def plot_accuracy(dae_accuracy, sparse_accuracy, logistic_accuracy):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Noms des algorithmes pour l'axe des x\n",
    "    algorithms = ['DAE', 'Sparse', 'Logistic Regression']\n",
    "    \n",
    "    # Précision moyenne pour chaque modèle\n",
    "    mean_accuracies = [\n",
    "        np.mean(dae_accuracy), \n",
    "        np.mean(sparse_accuracy), \n",
    "        logistic_accuracy\n",
    "    ]\n",
    "    \n",
    "    # Barres des précisions\n",
    "    plt.bar(algorithms, mean_accuracies, color=['blue', 'green', 'red'], alpha=0.7)\n",
    "    \n",
    "    plt.title('Accuracy Comparison of Different Models')\n",
    "    plt.xlabel('Algorithms')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)  # Set y-axis limit to 0-1 for accuracy representation\n",
    "    plt.grid(axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Exécution de l'entraînement et de la visualisation\n",
    "dae_history, sparse_history = train_models(X_train, X_val)\n",
    "logistic_model = train_logistic_regression(X_train, y_train)\n",
    "\n",
    "# Simulated accuracy values for DAE and Sparse models\n",
    "dae_accuracy = np.random.rand(50) * 0.1 + 0.8  # Simulated accuracy values for DAE\n",
    "sparse_accuracy = np.random.rand(50) * 0.1 + 0.75  # Simulated accuracy values for Sparse\n",
    "logistic_predictions = logistic_model.predict(X_val)\n",
    "logistic_accuracy = accuracy_score(y_val, logistic_predictions)  # Fixed accuracy for Logistic Regression\n",
    "\n",
    "# Visualiser les précisions\n",
    "plot_accuracy(dae_accuracy, sparse_accuracy, logistic_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Génération de données simulées\n",
    "def generate_data(num_samples=1000, num_features=10):\n",
    "    X = np.random.rand(num_samples, num_features)\n",
    "    noise = np.random.normal(0, 0.1, X.shape)\n",
    "    X_noisy = X + noise\n",
    "    \n",
    "    # Simulating binary labels (0 or 1) based on a threshold\n",
    "    y = (X.mean(axis=1) > 0.5).astype(int)\n",
    "    return X_noisy, X, y\n",
    "\n",
    "# Charger les données\n",
    "X_noisy, X, y = generate_data()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_noisy, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modèle DAE\n",
    "def build_dae_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Modèle épars\n",
    "def build_sparse_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(64, activation='relu', activity_regularizer=keras.regularizers.l1(0.01)),\n",
    "        layers.Dense(32, activation='relu', activity_regularizer=keras.regularizers.l1(0.01)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Modèle Deep\n",
    "def build_deep_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(input_shape[0], activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Entraînement des modèles\n",
    "def train_models(X_train, X_val):\n",
    "    input_shape = (X_train.shape[1],)\n",
    "\n",
    "    # DAE\n",
    "    dae_model = build_dae_model(input_shape)\n",
    "    dae_model.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_val, X_val), verbose=0)\n",
    "\n",
    "    # Modèle épars\n",
    "    sparse_model = build_sparse_model(input_shape)\n",
    "    sparse_model.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_val, X_val), verbose=0)\n",
    "\n",
    "    # Modèle Deep\n",
    "    deep_model = build_deep_model(input_shape)\n",
    "    deep_model.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_val, X_val), verbose=0)\n",
    "\n",
    "    return dae_model, sparse_model, deep_model\n",
    "\n",
    "# Entraîner les modèles de classification\n",
    "def train_classifiers(X_train, y_train):\n",
    "    models = {\n",
    "        \"Decision Tree\": DecisionTreeClassifier(),\n",
    "        \"Random Forest\": RandomForestClassifier(),\n",
    "        \"KNN\": KNeighborsClassifier(),\n",
    "        \"SVM\": SVC()\n",
    "    }\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Évaluer les modèles\n",
    "def evaluate_models(models, X_val, y_val):\n",
    "    accuracies = {}\n",
    "    for model_name, model in models.items():\n",
    "        predictions = model.predict(X_val)\n",
    "        accuracies[model_name] = accuracy_score(y_val, predictions)\n",
    "    return accuracies\n",
    "\n",
    "# Visualisation des précisions\n",
    "def plot_accuracy(accuracies_per_encoder):\n",
    "    encoders = list(accuracies_per_encoder.keys())\n",
    "    x = ['Decision Tree', 'Random Forest', 'KNN', 'SVM']\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, encoder in enumerate(encoders):\n",
    "        accuracies = accuracies_per_encoder[encoder]\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.bar(x, accuracies.values(), color=['blue', 'green', 'red', 'purple'], alpha=0.7)\n",
    "        \n",
    "        plt.title(f'Accuracy for {encoder}')\n",
    "        plt.xlabel('Algorithms')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.ylim(0, 1)  # Set y-axis limit to 0-1 for accuracy representation\n",
    "        plt.grid(axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Exécution de l'entraînement et de l'évaluation\n",
    "dae_model, sparse_model, deep_model = train_models(X_train, X_val)\n",
    "\n",
    "# Entraîner les modèles de classification sur les données transformées par chaque encodeur\n",
    "models_per_encoder = {\n",
    "    \"DAE\": train_classifiers(dae_model.predict(X_train), y_train),\n",
    "    \"Sparse\": train_classifiers(sparse_model.predict(X_train), y_train),\n",
    "    \"Deep\": train_classifiers(deep_model.predict(X_train), y_train)\n",
    "}\n",
    "\n",
    "# Évaluer les modèles\n",
    "accuracies_per_encoder = {encoder: evaluate_models(models, X_val, y_val) for encoder, models in models_per_encoder.items()}\n",
    "\n",
    "# Visualiser les précisions\n",
    "plot_accuracy(accuracies_per_encoder)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
